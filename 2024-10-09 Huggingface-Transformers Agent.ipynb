{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3845815e",
   "metadata": {},
   "source": [
    "# Orchestrating Huggingface Transformers Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7087a6a",
   "metadata": {},
   "source": [
    "### First install all relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b9c607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --upgrade huggingface_hub transformers[agents] duckduckgo-search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ab62a9",
   "metadata": {},
   "source": [
    "### Then import the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7bade59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.agents import HfApiEngine, ReactJsonAgent, ReactCodeAgent, DuckDuckGoSearchTool, ManagedAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90ca738b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login, InferenceClient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26244ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, json, os, re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adbfdeb",
   "metadata": {},
   "source": [
    "### Login to Huggingface to use their architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c517000d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: read).\n",
      "Your token has been saved in your configured git credential helpers (stor).\n",
      "Your token has been saved to /home/moebius/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "hf_token = \"<YOURTOKEN\"\n",
    "login(hf_token,add_to_git_credential=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd0d48b",
   "metadata": {},
   "source": [
    "### Now define the repositity you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b91ac04",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = \"Qwen/Qwen2.5-72B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd961da",
   "metadata": {},
   "source": [
    "## Select the right remote engine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3ee4b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_engine = HfApiEngine(model=repo_id)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9ffb43",
   "metadata": {},
   "source": [
    "## And setup the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612ae8e1",
   "metadata": {},
   "source": [
    "#####    How can I build an agent?\n",
    "\n",
    "To initialize an agent, you need these arguments:\n",
    "\n",
    "   1. an LLM to power your agent (Mistral in this case)- the agent is not exactly the LLM, it’s more like the agent is a program that uses an LLM as its engine.\n",
    "   2. a system prompt: what the LLM engine will be prompted with to generate its output\n",
    "   3.  a toolbox from which the agent pick tools to execute\n",
    "   4.  a parser to extract from the LLM output which tools are to call and with which arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e32ef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sp=\"\"\"Explain your reasoning step by step\"\"\"\n",
    "#system_prompt=sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92c9781b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"SERPAPI_API_KEY\"]=\"<SERPAPIKEY>\" #Not really used yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c5847cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_tools=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "646c5225",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32;20;1m======== New task ========\u001b[0m\n",
      "\u001b[37;1mWho is the CEO of Hugging Face?\u001b[0m\n",
      "\u001b[33;1m=== Agent thoughts:\u001b[0m\n",
      "\u001b[0mThought: I will use the `web_search` team member to find the current CEO of Hugging Face.\u001b[0m\n",
      "\u001b[33;1m>>> Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7mweb_search\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;7mrequest\u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144mWho is the current CEO of Hugging Face?\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[32;20;1m======== New task ========\u001b[0m\n",
      "\u001b[37;1mYou're a helpful agent named 'web_search'.\n",
      "You have been submitted this task by your manager.\n",
      "---\n",
      "Task:\n",
      "Who is the current CEO of Hugging Face?\n",
      "---\n",
      "You're helping your manager solve a wider task: so make sure to not provide a one-line answer, but give as much information as possible so that they have a clear understanding of the answer.\n",
      "\n",
      "Your final_answer WILL HAVE to contain these parts:\n",
      "### 1. Task outcome (short version):\n",
      "### 2. Task outcome (extremely detailed version):\n",
      "### 3. Additional context (if relevant):\n",
      "\n",
      "Put all these in your final_answer tool, everything that you do not pass as an argument to final_answer will be lost.\n",
      "And even if your task resolution is not successful, please return as much context as possible, so that your manager can act upon this feedback.\u001b[0m\n",
      "\u001b[33;1m=== Agent thoughts:\u001b[0m\n",
      "\u001b[0mThought: I will start by using the web_search tool to find out who the current CEO of Hugging Face is.\u001b[0m\n",
      "\u001b[33;1m>>> Calling tool: 'web_search' with arguments: {'query': 'current CEO of Hugging Face'}\u001b[0m\n",
      "\u001b[33;1m=== Agent thoughts:\u001b[0m\n",
      "\u001b[0mThought: Based on the search results, I have found that Clément Delangue is the CEO of Hugging Face. I will now provide this information in the required format.\u001b[0m\n",
      "\u001b[33;1m>>> Calling tool: 'final_answer' with arguments: {'answer': \"### 1. Task outcome (short version):\\nClément Delangue is the current CEO of Hugging Face.\\n\\n### 2. Task outcome (extremely detailed version):\\nClément Delangue, a co-founder of Hugging Face, serves as the CEO of the company. Hailing from La Bassée, France, Delangue has been an entrepreneur from a young age, even becoming one of the most prominent French sellers on eBay at 17. He pursued his education at Stanford University, earning a Master in Management degree. Hugging Face, a leading AI company focused on natural language processing, is valued at $4.5 billion as of the latest funding round, which raised $235 million. Delangue's leadership has been pivotal in Hugging Face's growth, which now has a team of 170 employees.\\n\\n### 3. Additional context (if relevant):\\n- Hugging Face is an AI startup that creates and hosts artificial intelligence software for other companies.\\n- The company has received significant investments from notable entities such as Google, Nvidia, and Salesforce.\\n- Hugging Face is actively involved in AI research and development, contributing to the open-source AI community and forming partnerships with organizations like IBM and NASA.\"}\u001b[0m\n",
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20m\u001b[0m\n",
      "\u001b[33;1mLast output from code snippet:\u001b[0m\n",
      "\u001b[32;20m### 1. Task outcome (short version):\n",
      "Clément Delangue is the current CEO of Hugging Face.\n",
      "\n",
      "### 2. Task outcome (extremely detailed version):\n",
      "Clément Delangue, a co-founder of Hugging Face, serves as the CEO of the company. Hailing from La Bassée, France, Delangue has been an entrepreneur from a young age, even becoming one of the most prominent French sellers on eBay at 17. He pursued his education at Stanford University, earning a Master in Management degree. Hugging Face, a leading AI company focused on natural language processing, is valued at $4.5 billion as of the latest funding round, which raised $235 million. Delangue's leadership has been pivotal in Hugging Face's growth, which now has a team of 170 employees.\n",
      "\n",
      "### 3. Additional context (if relevant):\n",
      "- Hugging Face is an AI startup that creates and hosts artificial intelligence software for other companies.\n",
      "- The company has received significant investments from notable entities such as Google, Nvidia, and Salesforce.\n",
      "- Hugging Face is actively involved in AI research and development, contributing to the open-source AI community and forming partnerships with organizations like IBM and NASA.\u001b[0m\n",
      "\u001b[33;1m=== Agent thoughts:\u001b[0m\n",
      "\u001b[0mThought: I have the information I need to return the final answer.\u001b[0m\n",
      "\u001b[33;1m>>> Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7mfinal_answer\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144mClément Delangue\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20m\u001b[0m\n",
      "\u001b[33;1mLast output from code snippet:\u001b[0m\n",
      "\u001b[32;20mClément Delangue\u001b[0m\n",
      "\u001b[32;20;1mFinal answer:\u001b[0m\n",
      "\u001b[32;20mClément Delangue\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Clément Delangue'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_agent = ReactJsonAgent(tools=[DuckDuckGoSearchTool()], llm_engine=llm_engine)\n",
    "\n",
    "managed_web_agent = ManagedAgent(\n",
    "    agent=web_agent,\n",
    "    name=\"web_search\",\n",
    "    description=\"Executes the DuckDuckGo Search Tool\"\n",
    ")\n",
    "\n",
    "manager_agent = ReactCodeAgent(\n",
    "    tools=[], llm_engine=llm_engine, managed_agents=[managed_web_agent]\n",
    ")\n",
    "\n",
    "manager_agent.run(\"Who is the CEO of Hugging Face?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "954b2c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32;20;1m======== New task ========\u001b[0m\n",
      "\u001b[37;1mHow many more blocks (also denoted as layers) in BERT base encoder than the encoder from the architecture proposed in Attention is All You Need?\u001b[0m\n",
      "\u001b[33;1m=== Agent thoughts:\u001b[0m\n",
      "\u001b[0mThought: To solve the problem, I first need to find out how many blocks (or layers) are in the BERT base encoder and how many are in the encoder architecture proposed in \"Attention is All You Need\". I will search the web for this information using the `web_search` tool.\u001b[0m\n",
      "\u001b[33;1m>>> Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7mweb_search\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;7mrequest\u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144mnumber of layers in BERT base encoder\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[32;20;1m======== New task ========\u001b[0m\n",
      "\u001b[37;1mYou're a helpful agent named 'web_search'.\n",
      "You have been submitted this task by your manager.\n",
      "---\n",
      "Task:\n",
      "number of layers in BERT base encoder\n",
      "---\n",
      "You're helping your manager solve a wider task: so make sure to not provide a one-line answer, but give as much information as possible so that they have a clear understanding of the answer.\n",
      "\n",
      "Your final_answer WILL HAVE to contain these parts:\n",
      "### 1. Task outcome (short version):\n",
      "### 2. Task outcome (extremely detailed version):\n",
      "### 3. Additional context (if relevant):\n",
      "\n",
      "Put all these in your final_answer tool, everything that you do not pass as an argument to final_answer will be lost.\n",
      "And even if your task resolution is not successful, please return as much context as possible, so that your manager can act upon this feedback.\u001b[0m\n",
      "\u001b[33;1m=== Agent thoughts:\u001b[0m\n",
      "\u001b[0mThought: To answer this question, I need to perform a web search to find specific information about the BERT base encoder model, such as the number of layers it contains.\u001b[0m\n",
      "\u001b[33;1m>>> Calling tool: 'web_search' with arguments: {'query': 'number of layers in BERT base encoder'}\u001b[0m\n",
      "\u001b[33;1m=== Agent thoughts:\u001b[0m\n",
      "\u001b[0mThought: I found several sources that provide the information we need about the number of layers in the BERT base encoder. I will extract the relevant details and provide a detailed answer.\u001b[0m\n",
      "\u001b[33;1m>>> Calling tool: 'final_answer' with arguments: {'answer': '### 1. Task outcome (short version):\\n\\nThe BERT base encoder has 12 layers.\\n\\n### 2. Task outcome (extremely detailed version):\\n\\nThe BERT base encoder is a specific variant of the BERT (Bidirectional Encoder Representations from Transformers) model. The BERT base model is designed with the following architecture:\\n- **Number of Layers**: 12\\n- **Hidden Nodes**: 768\\n- **Attention Heads**: 12\\n- **Total Parameters**: Approximately 110 million\\n\\nThese specifications are consistent across multiple sources, including:\\n- **GeeksforGeeks**: BERT BASE has 12 layers in the Encoder stack.\\n- **Medium**: BERT-Base has 12 layers, 768 hidden nodes, 12 attention heads, and 110M parameters.\\n- **Hugging Face**: The `num_hidden_layers` parameter is set to 12 by default for the BERT base model.\\n- **Wikipedia**: Although it does not specify the exact number of layers, it confirms that BERT uses the encoder-only transformer architecture.\\n\\n### 3. Additional context (if relevant):\\n\\n- **BERT LARGE**: In contrast to BERT base, the BERT LARGE model has 24 layers, 1024 hidden nodes, 16 attention heads, and approximately 340 million parameters. This makes BERT LARGE more resource-intensive but potentially more powerful for complex NLP tasks.\\n- **Training and Fine-Tuning**: The BERT base model is often used for pre-training on large datasets and then fine-tuned on smaller, task-specific datasets to achieve state-of-the-art performance in various NLP tasks.\\n- **Applications**: BERT base is widely used for tasks such as sentiment analysis, named entity recognition, question answering, and more.'}\u001b[0m\n",
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20m\u001b[0m\n",
      "\u001b[33;1mLast output from code snippet:\u001b[0m\n",
      "\u001b[32;20m### 1. Task outcome (short version):\n",
      "\n",
      "The BERT base encoder has 12 layers.\n",
      "\n",
      "### 2. Task outcome (extremely detailed version):\n",
      "\n",
      "The BERT base encoder is a specific variant of the BERT (Bidirectional Encoder Representations from Transformers) model. The BERT base model is designed with the following architecture:\n",
      "- **Number of Layers**: 12\n",
      "- **Hidden Nodes**: 768\n",
      "- **Attention Heads**: 12\n",
      "- **Total Parameters**: Approximately 110 million\n",
      "\n",
      "These specifications are consistent across multiple sources, including:\n",
      "- **GeeksforGeeks**: BERT BASE has 12 layers in the Encoder stack.\n",
      "- **Medium**: BERT-Base has 12 layers, 768 hidden nodes, 12 attention heads, and 110M parameters.\n",
      "- **Hugging Face**: The `num_hidden_layers` parameter is set to 12 by default for the BERT base model.\n",
      "- **Wikipedia**: Although it does not specify the exact number of layers, it confirms that BERT uses the encoder-only transformer architecture.\n",
      "\n",
      "### 3. Additional context (if relevant):\n",
      "\n",
      "- **BERT LARGE**: In contrast to BERT base, the BERT LARGE model has 24 layers, 1024 hidden nodes, 16 attention heads, and approximately 340 million parameters. This makes BERT LARGE more resource-intensive but potentially more powerful for complex NLP tasks.\n",
      "- **Training and Fine-Tuning**: The BERT base model is often used for pre-training on large datasets and then fine-tuned on smaller, task-specific datasets to achieve state-of-the-art performance in various NLP tasks.\n",
      "- **Applications**: BERT base is widely used for tasks such as sentiment analysis, named entity recognition, question answering, and more.\u001b[0m\n",
      "\u001b[33;1m=== Agent thoughts:\u001b[0m\n",
      "\u001b[0mThought: I now know that the BERT base encoder has 12 layers. Next, I need to find the number of layers in the encoder proposed in the \"Attention is All You Need\" paper. I will use the `web_search` tool again to find this information.\u001b[0m\n",
      "\u001b[33;1m>>> Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7mweb_search\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;7mrequest\u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144mnumber of layers in the encoder from Attention is All You Need\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[32;20;1m======== New task ========\u001b[0m\n",
      "\u001b[37;1mYou're a helpful agent named 'web_search'.\n",
      "You have been submitted this task by your manager.\n",
      "---\n",
      "Task:\n",
      "number of layers in the encoder from Attention is All You Need\n",
      "---\n",
      "You're helping your manager solve a wider task: so make sure to not provide a one-line answer, but give as much information as possible so that they have a clear understanding of the answer.\n",
      "\n",
      "Your final_answer WILL HAVE to contain these parts:\n",
      "### 1. Task outcome (short version):\n",
      "### 2. Task outcome (extremely detailed version):\n",
      "### 3. Additional context (if relevant):\n",
      "\n",
      "Put all these in your final_answer tool, everything that you do not pass as an argument to final_answer will be lost.\n",
      "And even if your task resolution is not successful, please return as much context as possible, so that your manager can act upon this feedback.\u001b[0m\n",
      "\u001b[33;1m=== Agent thoughts:\u001b[0m\n",
      "\u001b[0mThought: I need to find information about the number of layers in the encoder from the paper \"Attention is All You Need.\" I will start with a web search.\u001b[0m\n",
      "\u001b[33;1m>>> Calling tool: 'web_search' with arguments: {'query': 'number of layers in encoder Attention is All You Need'}\u001b[0m\n",
      "\u001b[33;1m=== Agent thoughts:\u001b[0m\n",
      "\u001b[0mThought: Based on the web search results, the number of layers in the encoder from the paper \"Attention is All You Need\" is 6. I will gather more details and provide a detailed answer.\u001b[0m\n",
      "\u001b[33;1m>>> Calling tool: 'final_answer' with arguments: {'answer': \"### 1. Task outcome (short version):\\n\\nThe number of layers in the encoder of the Transformer model as described in the paper 'Attention is All You Need' is 6.\\n\\n### 2. Task outcome (extremely detailed version):\\n\\nIn the paper 'Attention is All You Need' by Vaswani et al. (2017), the authors introduced the Transformer model, which relies solely on self-attention mechanisms to process sequences. The Transformer architecture is composed of an encoder and a decoder, both of which are built from identical, stacked layers. Specifically, the encoder consists of 6 identical layers, each of which is composed of two sub-layers:\\n\\n1. **Multi-Head Self-Attention Layer**: This layer is responsible for capturing dependencies between different positions in the input sequence. It uses multiple attention heads to allow the model to focus on different aspects of the input simultaneously.\\n\\n2. **Feed-Forward Neural Network (FFN) Layer**: This is a fully connected feed-forward network applied to each position separately and identically. The FFN consists of two linear transformations with a ReLU activation function in between.\\n\\nEach sub-layer is followed by a residual connection, which is a skipped connection around the sub-layer, and a layer normalization step to stabilize and improve the training process.\\n\\n### 3. Additional context (if relevant):\\n\\n- **Number of Attention Heads**: The paper specifies that each multi-head self-attention layer in the encoder uses 8 attention heads. This allows the model to capture different aspects of the input sequence from multiple perspectives.\\n\\n- **Feed-Forward Network Dimensions**: The feed-forward network in each layer has the same hidden dimension throughout the model. In the original paper, this dimension is set to 2048, and the input/output embedding dimensions are 512.\\n\\n- **Layer Normalization**: The layer normalization is applied after each sub-layer and before the residual connection. This helps in stabilizing the learning process and improving the model's performance.\\n\\n- **Positional Encoding**: Since the model does not use recurrence, positional encoding is added to the input embeddings to provide information about the position of each token in the sequence.\\n\\n- **Model Architecture**: The overall architecture of the Transformer includes the encoder, decoder, and a linear layer followed by a softmax function for the output. The encoder processes the input sequence, and the decoder generates the output sequence, with both components using self-attention mechanisms to capture long-range dependencies.\\n\\nThis architecture has been highly influential in the field of natural language processing and has led to many subsequent advancements in transformer-based models.\"}\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20m\u001b[0m\n",
      "\u001b[33;1mLast output from code snippet:\u001b[0m\n",
      "\u001b[32;20m### 1. Task outcome (short version):\n",
      "\n",
      "The number of layers in the encoder of the Transformer model as described in the paper 'Attention is All You Need' is 6.\n",
      "\n",
      "### 2. Task outcome (extremely detailed version):\n",
      "\n",
      "In the paper 'Attention is All You Need' by Vaswani et al. (2017), the authors introduced the Transformer model, which relies solely on self-attention mechanisms to process sequences. The Transformer architecture is composed of an encoder and a decoder, both of which are built from identical, stacked layers. Specifically, the encoder consists of 6 identical layers, each of which is composed of two sub-layers:\n",
      "\n",
      "1. **Multi-Head Self-Attention Layer**: This layer is responsible for capturing dependencies between different positions in the input sequence. It uses multiple attention heads to allow the model to focus on different aspects of the input simultaneously.\n",
      "\n",
      "2. **Feed-Forward Neural Network (FFN) Layer**: This is a fully connected feed-forward network applied to each position separately and identically. The FFN consists of two linear transformations with a ReLU activation function in between.\n",
      "\n",
      "Each sub-layer is followed by a residual connection, which is a skipped connection around the sub-layer, and a layer normalization step to stabilize and improve the training process.\n",
      "\n",
      "### 3. Additional context (if relevant):\n",
      "\n",
      "- **Number of Attention Heads**: The paper specifies that each multi-head self-attention layer in the encoder uses 8 attention heads. This allows the model to capture different aspects of the input sequence from multiple perspectives.\n",
      "\n",
      "- **Feed-Forward Network Dimensions**: The feed-forward network in each layer has the same hidden dimension throughout the model. In the original paper, this dimension is set to 2048, and the input/output embedding dimensions are 512.\n",
      "\n",
      "- **Layer Normalization**: The layer normalization is applied after each sub-layer and before the residual connection. This helps in stabilizing the learning process and improving the model's performance.\n",
      "\n",
      "- **Positional Encoding**: Since the model does not use recurrence, positional encoding is added to the input embeddings to provide information about the position of each token in the sequence.\n",
      "\n",
      "- **Model Architecture**: The overall architecture of the Transformer includes the encoder, decoder, and a linear layer followed by a softmax function for the output. The encoder processes the input sequence, and the decoder generates the output sequence, with both components using self-attention mechanisms to capture long-range dependencies.\n",
      "\n",
      "This architecture has been highly influential in the field of natural language processing and has led to many subsequent advancements in transformer-based models.\u001b[0m\n",
      "\u001b[33;1m=== Agent thoughts:\u001b[0m\n",
      "\u001b[0mThought: I now know that the Transformer encoder proposed in \"Attention is All You Need\" has 6 layers. Since I already know that the BERT base encoder has 12 layers, I can now compute the difference and return the final answer.\u001b[0m\n",
      "\u001b[33;1m>>> Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7mbert_base_layers\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m12\u001b[39m\n",
      "\u001b[38;5;7mtransformer_layers\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m6\u001b[39m\n",
      "\u001b[38;5;7mlayers_difference\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mbert_base_layers\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m-\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mtransformer_layers\u001b[39m\n",
      "\u001b[38;5;7mfinal_answer\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;7mlayers_difference\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20m\u001b[0m\n",
      "\u001b[33;1mLast output from code snippet:\u001b[0m\n",
      "\u001b[32;20m6\u001b[0m\n",
      "\u001b[32;20;1mFinal answer:\u001b[0m\n",
      "\u001b[32;20m6\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager_agent.run(\"How many more blocks (also denoted as layers) in BERT base encoder than the encoder from the architecture proposed in Attention is All You Need?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e654f009",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
