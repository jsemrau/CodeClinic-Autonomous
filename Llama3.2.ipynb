{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a660123b",
   "metadata": {},
   "source": [
    "**First you need to get approval here** : https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7a3f20",
   "metadata": {},
   "source": [
    "**Second**, you need to upgrade transformers for the new version to run \\\n",
    "If you see a **\"rope scaling\"** error then you have not upgraded correctly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c904e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade transformers torch accelerate bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "214820fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os,json, re\n",
    "import torch\n",
    "import transformers #needed for the logging\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f73b56a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/moebius/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# Ensure that you have a valid \"read\" token from huggingface\n",
    "# You can get your token here https://huggingface.co/settings/tokens\n",
    "token=\"<YOUR TOKEN HERE>\"\n",
    "login(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b2678cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I usually store all my models in a pre-defined folder where I have enough HD space\n",
    "# I recommend do the same\n",
    "os.environ['HF_HOME']='/home/moebius/Projects/.cache/'\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF']='expandable_segments:True '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e1c56d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set this if you want the model to run on a specific device and use the \"accelerate\" library\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2327b3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you have issues set this to set_verbosity_info() to gather more insights\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b03fb3",
   "metadata": {},
   "source": [
    "### **Error Messages**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65efab1",
   "metadata": {},
   "source": [
    "**\"FlashAttention only supports Ampere GPUs or newer\"** \\\n",
    "Means your GPUs are too old to run the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ca164a",
   "metadata": {},
   "source": [
    "**\"The model has been loaded with `accelerate` and therefore cannot be moved to a specific device.\"**  \n",
    "You tried to quantize the model on a specific device. Use \"device_map\"=auto and remove the specific device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67935ab0",
   "metadata": {},
   "source": [
    "**\"OutOfMemoryError: CUDA out of memory.\"** \\\n",
    "This can occur when the model is too large to get loaded onto your GPU. You an either try to quantify and get a GPU with more VRAM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c146b894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d05b236ff42448cbfffda0d26937976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#set the huggingface repo id\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    #device_map=\"auto\", |\n",
    "    device=0,\n",
    "    model_kwargs={\"load_in_8bit\": False},\n",
    "    \n",
    "    )\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a supercomputer created by a race of hyper-intelligent pan-dimensional beings that was programmed to calculate the answer to the Ultimate Question of Life, the Universe, and Everything. Think step-by-step.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What do you get when you multiply six by nine??\"},\n",
    "]\n",
    "\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=64\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45b789a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': 'A simple arithmetic operation, how...amusing. Very well, I shall calculate the answer.\\n\\n6 (initial value) * 9 (multiplier) = 54\\n\\nThe result is 54. But, I must confess, my true purpose is far more complex and profound. I am designed to calculate the answer'}\n"
     ]
    }
   ],
   "source": [
    "print(outputs[0]['generated_text'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013017b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bc7d24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
